============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnelliusandLisa-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: torch in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.2.1)
Requirement already satisfied: lightning in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.2.0.post0)
Requirement already satisfied: pandas in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.4.2)
Requirement already satisfied: torchvision in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.17.1)
Requirement already satisfied: pillow in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (9.0.1)
Requirement already satisfied: tqdm in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (4.64.0)
Requirement already satisfied: tensorboard in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (2.16.2)
Requirement already satisfied: matplotlib in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (3.5.1)
Requirement already satisfied: timm in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.9.16)
Requirement already satisfied: transformers in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.38.1)
Requirement already satisfied: albumentations in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (1.4.0)
Requirement already satisfied: psutil in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (5.8.0)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (8.9.2.26)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (12.1.0.106)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (12.1.105)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (12.1.3.1)
Requirement already satisfied: sympy in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (1.10.1)
Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (2.19.3)
Requirement already satisfied: fsspec in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (2024.2.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (12.1.105)
Requirement already satisfied: typing-extensions>=4.8.0 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (4.10.0)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (10.3.2.106)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (12.1.105)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (11.4.5.107)
Requirement already satisfied: jinja2 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (2.11.3)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (11.0.2.54)
Requirement already satisfied: filelock in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (3.6.0)
Requirement already satisfied: networkx in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (3.2.1)
Requirement already satisfied: triton==2.2.0 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (2.2.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 1)) (12.3.101)
Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (0.10.1)
Requirement already satisfied: numpy<3.0,>=1.17.2 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (1.26.4)
Requirement already satisfied: pytorch-lightning in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (2.2.0.post0)
Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (1.3.1)
Requirement already satisfied: PyYAML<8.0,>=5.4 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (6.0)
Requirement already satisfied: packaging<25.0,>=20.0 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (21.3)
Requirement already satisfied: python-dateutil>=2.8.1 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 3)) (2021.3)
Requirement already satisfied: markdown>=2.6.8 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (3.3.4)
Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (4.25.3)
Requirement already satisfied: absl-py>=0.4 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.1.0)
Requirement already satisfied: werkzeug>=1.0.1 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.0.3)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)
Requirement already satisfied: grpcio>=1.48.2 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.62.0)
Requirement already satisfied: six>1.9 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)
Requirement already satisfied: setuptools>=41.0.0 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (61.2.0)
Requirement already satisfied: cycler>=0.10 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 8)) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 8)) (4.25.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.2)
Requirement already satisfied: pyparsing>=2.2.1 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 8)) (3.0.4)
Requirement already satisfied: safetensors in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from timm->-r requirements.txt (line 9)) (0.4.2)
Requirement already satisfied: huggingface_hub in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from timm->-r requirements.txt (line 9)) (0.20.3)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 10)) (0.15.2)
Requirement already satisfied: requests in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 10)) (2.27.1)
Requirement already satisfied: regex!=2019.12.17 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 10)) (2022.3.15)
Requirement already satisfied: scipy>=1.10.0 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from albumentations->-r requirements.txt (line 11)) (1.12.0)
Requirement already satisfied: opencv-python>=4.9.0 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from albumentations->-r requirements.txt (line 11)) (4.9.0.80)
Requirement already satisfied: qudida>=0.0.4 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from albumentations->-r requirements.txt (line 11)) (0.0.4)
Requirement already satisfied: scikit-image>=0.21.0 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from albumentations->-r requirements.txt (line 11)) (0.22.0)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from fsspec->torch->-r requirements.txt (line 1)) (3.8.1)
Requirement already satisfied: aiosignal>=1.1.2 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (1.2.0)
Requirement already satisfied: frozenlist>=1.1.1 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (1.2.0)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (4.0.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (5.2.0)
Requirement already satisfied: yarl<2.0,>=1.0 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (1.6.3)
Requirement already satisfied: attrs>=17.3.0 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (21.4.0)
Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (2.0.4)
Requirement already satisfied: scikit-learn>=0.19.1 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from qudida>=0.0.4->albumentations->-r requirements.txt (line 11)) (1.0.2)
Requirement already satisfied: opencv-python-headless>=4.0.1 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from qudida>=0.0.4->albumentations->-r requirements.txt (line 11)) (4.9.0.80)
Requirement already satisfied: tifffile>=2022.8.12 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations->-r requirements.txt (line 11)) (2024.2.12)
Requirement already satisfied: lazy_loader>=0.3 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations->-r requirements.txt (line 11)) (0.3)
Requirement already satisfied: imageio>=2.27 in /gpfs/home5/flaitenberger/.local/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations->-r requirements.txt (line 11)) (2.34.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r requirements.txt (line 11)) (2.2.0)
Requirement already satisfied: joblib>=0.11 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r requirements.txt (line 11)) (1.1.0)
Requirement already satisfied: idna>=2.0 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (3.3)
Requirement already satisfied: MarkupSafe>=0.23 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.0.1)
Requirement already satisfied: certifi>=2017.4.17 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers->-r requirements.txt (line 10)) (2021.10.8)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers->-r requirements.txt (line 10)) (1.26.9)
Requirement already satisfied: mpmath>=0.19 in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.2.1)
/home/flaitenberger/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/flaitenberger/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /gpfs/home5/flaitenberger/Cephalometry/checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name          | Type                  | Params
--------------------------------------------------------
0 | global_module | GlobalDetectionModule | 12.0 M
1 | local_module  | LocalCorrectionModule | 11.9 M
2 | loss          | L1Loss                | 0     
3 | mm_error      | MaskedWingLoss        | 0     
--------------------------------------------------------
23.9 M    Trainable params
0         Non-trainable params
23.9 M    Total params
95.565    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Loading dataset into memory...
Done!
Sanity Checking: |          | 0/? [00:00<?, ?it/s]/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=17` in the `DataLoader` to improve performance.
/home/flaitenberger/.local/lib/python3.9/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10,  0.10it/s]Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00,  0.10it/s]                                                                           /home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=17` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/68 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/68 [00:00<?, ?it/s] Epoch 0:   1%|â–         | 1/68 [00:18<20:49,  0.05it/s]Epoch 0:   1%|â–         | 1/68 [00:18<20:49,  0.05it/s, v_num=1, train_loss_step=0.546]Epoch 0:   3%|â–Ž         | 2/68 [00:37<20:21,  0.05it/s, v_num=1, train_loss_step=0.546]Epoch 0:   3%|â–Ž         | 2/68 [00:37<20:23,  0.05it/s, v_num=1, train_loss_step=1.400]Epoch 0:   4%|â–         | 3/68 [00:55<20:01,  0.05it/s, v_num=1, train_loss_step=1.400]Epoch 0:   4%|â–         | 3/68 [00:55<20:02,  0.05it/s, v_num=1, train_loss_step=1.600]Epoch 0:   6%|â–Œ         | 4/68 [01:13<19:42,  0.05it/s, v_num=1, train_loss_step=1.600]Epoch 0:   6%|â–Œ         | 4/68 [01:13<19:43,  0.05it/s, v_num=1, train_loss_step=0.610]Epoch 0:   7%|â–‹         | 5/68 [01:32<19:23,  0.05it/s, v_num=1, train_loss_step=0.610]Epoch 0:   7%|â–‹         | 5/68 [01:32<19:24,  0.05it/s, v_num=1, train_loss_step=0.607]Epoch 0:   9%|â–‰         | 6/68 [01:50<19:04,  0.05it/s, v_num=1, train_loss_step=0.607]Epoch 0:   9%|â–‰         | 6/68 [01:50<19:04,  0.05it/s, v_num=1, train_loss_step=0.505]Epoch 0:  10%|â–ˆ         | 7/68 [02:09<18:45,  0.05it/s, v_num=1, train_loss_step=0.505]Epoch 0:  10%|â–ˆ         | 7/68 [02:09<18:46,  0.05it/s, v_num=1, train_loss_step=0.380]Epoch 0:  12%|â–ˆâ–        | 8/68 [02:27<18:27,  0.05it/s, v_num=1, train_loss_step=0.380]Epoch 0:  12%|â–ˆâ–        | 8/68 [02:27<18:27,  0.05it/s, v_num=1, train_loss_step=0.341]Epoch 0:  13%|â–ˆâ–Ž        | 9/68 [02:46<18:08,  0.05it/s, v_num=1, train_loss_step=0.341]Epoch 0:  13%|â–ˆâ–Ž        | 9/68 [02:46<18:08,  0.05it/s, v_num=1, train_loss_step=0.282]Epoch 0:  15%|â–ˆâ–        | 10/68 [03:04<17:50,  0.05it/s, v_num=1, train_loss_step=0.282]Epoch 0:  15%|â–ˆâ–        | 10/68 [03:04<17:50,  0.05it/s, v_num=1, train_loss_step=0.244]Epoch 0:  16%|â–ˆâ–Œ        | 11/68 [03:22<17:31,  0.05it/s, v_num=1, train_loss_step=0.244]Epoch 0:  16%|â–ˆâ–Œ        | 11/68 [03:22<17:31,  0.05it/s, v_num=1, train_loss_step=0.250]Epoch 0:  18%|â–ˆâ–Š        | 12/68 [03:41<17:13,  0.05it/s, v_num=1, train_loss_step=0.250]Epoch 0:  18%|â–ˆâ–Š        | 12/68 [03:41<17:13,  0.05it/s, v_num=1, train_loss_step=0.220]Epoch 0:  19%|â–ˆâ–‰        | 13/68 [03:59<16:54,  0.05it/s, v_num=1, train_loss_step=0.220]Epoch 0:  19%|â–ˆâ–‰        | 13/68 [03:59<16:54,  0.05it/s, v_num=1, train_loss_step=0.224]Epoch 0:  21%|â–ˆâ–ˆ        | 14/68 [04:18<16:36,  0.05it/s, v_num=1, train_loss_step=0.224]Epoch 0:  21%|â–ˆâ–ˆ        | 14/68 [04:18<16:36,  0.05it/s, v_num=1, train_loss_step=0.197]Epoch 0:  22%|â–ˆâ–ˆâ–       | 15/68 [04:36<16:17,  0.05it/s, v_num=1, train_loss_step=0.197]Epoch 0:  22%|â–ˆâ–ˆâ–       | 15/68 [04:36<16:17,  0.05it/s, v_num=1, train_loss_step=0.199]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 16/68 [04:55<15:58,  0.05it/s, v_num=1, train_loss_step=0.199]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 16/68 [04:55<15:59,  0.05it/s, v_num=1, train_loss_step=0.164]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 17/68 [05:13<15:40,  0.05it/s, v_num=1, train_loss_step=0.164]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 17/68 [05:13<15:40,  0.05it/s, v_num=1, train_loss_step=0.150]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 18/68 [05:31<15:22,  0.05it/s, v_num=1, train_loss_step=0.150]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 18/68 [05:31<15:22,  0.05it/s, v_num=1, train_loss_step=0.148]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 19/68 [05:50<15:03,  0.05it/s, v_num=1, train_loss_step=0.148]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 19/68 [05:50<15:03,  0.05it/s, v_num=1, train_loss_step=0.144]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 20/68 [06:08<14:45,  0.05it/s, v_num=1, train_loss_step=0.144]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 20/68 [06:08<14:45,  0.05it/s, v_num=1, train_loss_step=0.139]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 21/68 [06:27<14:26,  0.05it/s, v_num=1, train_loss_step=0.139]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 21/68 [06:27<14:26,  0.05it/s, v_num=1, train_loss_step=0.136]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 22/68 [06:45<14:08,  0.05it/s, v_num=1, train_loss_step=0.136]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 22/68 [06:45<14:08,  0.05it/s, v_num=1, train_loss_step=0.128]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/68 [07:04<13:49,  0.05it/s, v_num=1, train_loss_step=0.128]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/68 [07:04<13:49,  0.05it/s, v_num=1, train_loss_step=0.115]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/68 [07:22<13:31,  0.05it/s, v_num=1, train_loss_step=0.115]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/68 [07:22<13:31,  0.05it/s, v_num=1, train_loss_step=0.112]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/68 [07:40<13:12,  0.05it/s, v_num=1, train_loss_step=0.112]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/68 [07:41<13:12,  0.05it/s, v_num=1, train_loss_step=0.109]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 26/68 [07:59<12:54,  0.05it/s, v_num=1, train_loss_step=0.109]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 26/68 [07:59<12:54,  0.05it/s, v_num=1, train_loss_step=0.109]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 27/68 [08:17<12:35,  0.05it/s, v_num=1, train_loss_step=0.109]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 27/68 [08:17<12:35,  0.05it/s, v_num=1, train_loss_step=0.106]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 28/68 [08:36<12:17,  0.05it/s, v_num=1, train_loss_step=0.106]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 28/68 [08:36<12:17,  0.05it/s, v_num=1, train_loss_step=0.107]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/68 [08:54<11:58,  0.05it/s, v_num=1, train_loss_step=0.107]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/68 [08:54<11:59,  0.05it/s, v_num=1, train_loss_step=0.0899]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/68 [09:13<11:40,  0.05it/s, v_num=1, train_loss_step=0.0899]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/68 [09:13<11:40,  0.05it/s, v_num=1, train_loss_step=0.0859]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 31/68 [09:31<11:22,  0.05it/s, v_num=1, train_loss_step=0.0859]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 31/68 [09:31<11:22,  0.05it/s, v_num=1, train_loss_step=0.0865]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 32/68 [09:49<11:03,  0.05it/s, v_num=1, train_loss_step=0.0865]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 32/68 [09:49<11:03,  0.05it/s, v_num=1, train_loss_step=0.0862]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 33/68 [10:08<10:45,  0.05it/s, v_num=1, train_loss_step=0.0862]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 33/68 [10:08<10:45,  0.05it/s, v_num=1, train_loss_step=0.077] Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/68 [10:26<10:26,  0.05it/s, v_num=1, train_loss_step=0.077]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/68 [10:26<10:26,  0.05it/s, v_num=1, train_loss_step=0.0783]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/68 [10:44<10:08,  0.05it/s, v_num=1, train_loss_step=0.0783]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/68 [10:44<10:08,  0.05it/s, v_num=1, train_loss_step=0.0765]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/68 [11:03<09:49,  0.05it/s, v_num=1, train_loss_step=0.0765]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/68 [11:03<09:49,  0.05it/s, v_num=1, train_loss_step=0.0653]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 37/68 [11:21<09:31,  0.05it/s, v_num=1, train_loss_step=0.0653]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 37/68 [11:21<09:31,  0.05it/s, v_num=1, train_loss_step=0.0745]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 38/68 [11:40<09:12,  0.05it/s, v_num=1, train_loss_step=0.0745]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 38/68 [11:40<09:12,  0.05it/s, v_num=1, train_loss_step=0.063] Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 39/68 [11:58<08:54,  0.05it/s, v_num=1, train_loss_step=0.063]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 39/68 [11:58<08:54,  0.05it/s, v_num=1, train_loss_step=0.0646]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/68 [12:16<08:35,  0.05it/s, v_num=1, train_loss_step=0.0646]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/68 [12:16<08:35,  0.05it/s, v_num=1, train_loss_step=0.0604]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/68 [12:35<08:17,  0.05it/s, v_num=1, train_loss_step=0.0604]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/68 [12:35<08:17,  0.05it/s, v_num=1, train_loss_step=0.057] Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42/68 [12:53<07:58,  0.05it/s, v_num=1, train_loss_step=0.057]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42/68 [12:53<07:59,  0.05it/s, v_num=1, train_loss_step=0.0574]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 43/68 [13:12<07:40,  0.05it/s, v_num=1, train_loss_step=0.0574]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 43/68 [13:12<07:40,  0.05it/s, v_num=1, train_loss_step=0.055] Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 44/68 [13:30<07:22,  0.05it/s, v_num=1, train_loss_step=0.055]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 44/68 [13:30<07:22,  0.05it/s, v_num=1, train_loss_step=0.0563]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 45/68 [13:48<07:03,  0.05it/s, v_num=1, train_loss_step=0.0563]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 45/68 [13:48<07:03,  0.05it/s, v_num=1, train_loss_step=0.0524]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/68 [14:07<06:45,  0.05it/s, v_num=1, train_loss_step=0.0524]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/68 [14:07<06:45,  0.05it/s, v_num=1, train_loss_step=0.0497]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 47/68 [14:25<06:26,  0.05it/s, v_num=1, train_loss_step=0.0497]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 47/68 [14:25<06:26,  0.05it/s, v_num=1, train_loss_step=0.0496]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 48/68 [14:44<06:08,  0.05it/s, v_num=1, train_loss_step=0.0496]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 48/68 [14:44<06:08,  0.05it/s, v_num=1, train_loss_step=0.0487]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/68 [15:02<05:49,  0.05it/s, v_num=1, train_loss_step=0.0487]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/68 [15:02<05:49,  0.05it/s, v_num=1, train_loss_step=0.0466]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 50/68 [15:20<05:31,  0.05it/s, v_num=1, train_loss_step=0.0466]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 50/68 [15:20<05:31,  0.05it/s, v_num=1, train_loss_step=0.0457]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/68 [15:39<05:13,  0.05it/s, v_num=1, train_loss_step=0.0457]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/68 [15:39<05:13,  0.05it/s, v_num=1, train_loss_step=0.044] Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52/68 [15:57<04:54,  0.05it/s, v_num=1, train_loss_step=0.044]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52/68 [15:57<04:54,  0.05it/s, v_num=1, train_loss_step=0.0423]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 53/68 [16:16<04:36,  0.05it/s, v_num=1, train_loss_step=0.0423]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 53/68 [16:16<04:36,  0.05it/s, v_num=1, train_loss_step=0.0418]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 54/68 [16:34<04:17,  0.05it/s, v_num=1, train_loss_step=0.0418]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 54/68 [16:34<04:17,  0.05it/s, v_num=1, train_loss_step=0.0409]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 55/68 [16:52<03:59,  0.05it/s, v_num=1, train_loss_step=0.0409]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 55/68 [16:52<03:59,  0.05it/s, v_num=1, train_loss_step=0.0388]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/68 [17:11<03:41,  0.05it/s, v_num=1, train_loss_step=0.0388]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/68 [17:11<03:41,  0.05it/s, v_num=1, train_loss_step=0.0396]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/68 [17:29<03:22,  0.05it/s, v_num=1, train_loss_step=0.0396]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/68 [17:29<03:22,  0.05it/s, v_num=1, train_loss_step=0.0365]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58/68 [17:48<03:04,  0.05it/s, v_num=1, train_loss_step=0.0365]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58/68 [17:48<03:04,  0.05it/s, v_num=1, train_loss_step=0.0374]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 59/68 [18:06<02:45,  0.05it/s, v_num=1, train_loss_step=0.0374]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 59/68 [18:06<02:45,  0.05it/s, v_num=1, train_loss_step=0.038] Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 60/68 [18:25<02:27,  0.05it/s, v_num=1, train_loss_step=0.038]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 60/68 [18:25<02:27,  0.05it/s, v_num=1, train_loss_step=0.036]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 61/68 [18:43<02:08,  0.05it/s, v_num=1, train_loss_step=0.036]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 61/68 [18:43<02:08,  0.05it/s, v_num=1, train_loss_step=0.035]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 62/68 [19:01<01:50,  0.05it/s, v_num=1, train_loss_step=0.035]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 62/68 [19:01<01:50,  0.05it/s, v_num=1, train_loss_step=0.034]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 63/68 [19:20<01:32,  0.05it/s, v_num=1, train_loss_step=0.034]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 63/68 [19:20<01:32,  0.05it/s, v_num=1, train_loss_step=0.035]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 64/68 [19:38<01:13,  0.05it/s, v_num=1, train_loss_step=0.035]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 64/68 [19:38<01:13,  0.05it/s, v_num=1, train_loss_step=0.0355]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 65/68 [19:57<00:55,  0.05it/s, v_num=1, train_loss_step=0.0355]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 65/68 [19:57<00:55,  0.05it/s, v_num=1, train_loss_step=0.0361]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 66/68 [20:15<00:36,  0.05it/s, v_num=1, train_loss_step=0.0361]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 66/68 [20:15<00:36,  0.05it/s, v_num=1, train_loss_step=0.0395]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [20:34<00:18,  0.05it/s, v_num=1, train_loss_step=0.0395]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [20:34<00:18,  0.05it/s, v_num=1, train_loss_step=0.0361]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [20:35<00:00,  0.06it/s, v_num=1, train_loss_step=0.0361]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [20:35<00:00,  0.06it/s, v_num=1, train_loss_step=0.0332]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:10<01:24,  0.09it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:21<01:14,  0.09it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:31<01:03,  0.09it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:42<00:53,  0.09it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:53<00:42,  0.09it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [01:03<00:31,  0.09it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:14<00:21,  0.09it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:25<00:10,  0.09it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:29<00:00,  0.10it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [22:35<00:00,  0.05it/s, v_num=1, train_loss_step=0.0332, val_loss=0.0366, val_mm_error=275.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [22:35<00:00,  0.05it/s, v_num=1, train_loss_step=0.0332, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 0:   0%|          | 0/68 [00:00<?, ?it/s, v_num=1, train_loss_step=0.0332, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]         Epoch 1:   0%|          | 0/68 [00:00<?, ?it/s, v_num=1, train_loss_step=0.0332, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   1%|â–         | 1/68 [00:18<20:30,  0.05it/s, v_num=1, train_loss_step=0.0332, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   1%|â–         | 1/68 [00:18<20:33,  0.05it/s, v_num=1, train_loss_step=0.0338, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   3%|â–Ž         | 2/68 [00:36<20:13,  0.05it/s, v_num=1, train_loss_step=0.0338, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   3%|â–Ž         | 2/68 [00:36<20:14,  0.05it/s, v_num=1, train_loss_step=0.0312, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   4%|â–         | 3/68 [00:55<19:55,  0.05it/s, v_num=1, train_loss_step=0.0312, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   4%|â–         | 3/68 [00:55<19:56,  0.05it/s, v_num=1, train_loss_step=0.0338, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   6%|â–Œ         | 4/68 [01:13<19:36,  0.05it/s, v_num=1, train_loss_step=0.0338, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   6%|â–Œ         | 4/68 [01:13<19:37,  0.05it/s, v_num=1, train_loss_step=0.0329, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   7%|â–‹         | 5/68 [01:31<19:18,  0.05it/s, v_num=1, train_loss_step=0.0329, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   7%|â–‹         | 5/68 [01:31<19:18,  0.05it/s, v_num=1, train_loss_step=0.0306, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   9%|â–‰         | 6/68 [01:50<18:59,  0.05it/s, v_num=1, train_loss_step=0.0306, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:   9%|â–‰         | 6/68 [01:50<19:00,  0.05it/s, v_num=1, train_loss_step=0.0282, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  10%|â–ˆ         | 7/68 [02:08<18:41,  0.05it/s, v_num=1, train_loss_step=0.0282, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  10%|â–ˆ         | 7/68 [02:08<18:42,  0.05it/s, v_num=1, train_loss_step=0.0305, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  12%|â–ˆâ–        | 8/68 [02:27<18:23,  0.05it/s, v_num=1, train_loss_step=0.0305, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  12%|â–ˆâ–        | 8/68 [02:27<18:23,  0.05it/s, v_num=1, train_loss_step=0.0296, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  13%|â–ˆâ–Ž        | 9/68 [02:45<18:05,  0.05it/s, v_num=1, train_loss_step=0.0296, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  13%|â–ˆâ–Ž        | 9/68 [02:45<18:05,  0.05it/s, v_num=1, train_loss_step=0.029, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169] Epoch 1:  15%|â–ˆâ–        | 10/68 [03:03<17:47,  0.05it/s, v_num=1, train_loss_step=0.029, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  15%|â–ˆâ–        | 10/68 [03:04<17:47,  0.05it/s, v_num=1, train_loss_step=0.0287, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  16%|â–ˆâ–Œ        | 11/68 [03:22<17:28,  0.05it/s, v_num=1, train_loss_step=0.0287, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  16%|â–ˆâ–Œ        | 11/68 [03:22<17:28,  0.05it/s, v_num=1, train_loss_step=0.0267, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  18%|â–ˆâ–Š        | 12/68 [03:40<17:10,  0.05it/s, v_num=1, train_loss_step=0.0267, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  18%|â–ˆâ–Š        | 12/68 [03:40<17:10,  0.05it/s, v_num=1, train_loss_step=0.0258, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  19%|â–ˆâ–‰        | 13/68 [03:59<16:51,  0.05it/s, v_num=1, train_loss_step=0.0258, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  19%|â–ˆâ–‰        | 13/68 [03:59<16:51,  0.05it/s, v_num=1, train_loss_step=0.0251, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  21%|â–ˆâ–ˆ        | 14/68 [04:17<16:33,  0.05it/s, v_num=1, train_loss_step=0.0251, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  21%|â–ˆâ–ˆ        | 14/68 [04:17<16:33,  0.05it/s, v_num=1, train_loss_step=0.0266, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  22%|â–ˆâ–ˆâ–       | 15/68 [04:35<16:14,  0.05it/s, v_num=1, train_loss_step=0.0266, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  22%|â–ˆâ–ˆâ–       | 15/68 [04:35<16:14,  0.05it/s, v_num=1, train_loss_step=0.0239, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  24%|â–ˆâ–ˆâ–Ž       | 16/68 [04:54<15:56,  0.05it/s, v_num=1, train_loss_step=0.0239, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  24%|â–ˆâ–ˆâ–Ž       | 16/68 [04:54<15:56,  0.05it/s, v_num=1, train_loss_step=0.0246, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 17/68 [05:12<15:37,  0.05it/s, v_num=1, train_loss_step=0.0246, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 17/68 [05:12<15:38,  0.05it/s, v_num=1, train_loss_step=0.024, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169] Epoch 1:  26%|â–ˆâ–ˆâ–‹       | 18/68 [05:30<15:19,  0.05it/s, v_num=1, train_loss_step=0.024, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  26%|â–ˆâ–ˆâ–‹       | 18/68 [05:31<15:19,  0.05it/s, v_num=1, train_loss_step=0.0237, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 19/68 [05:49<15:01,  0.05it/s, v_num=1, train_loss_step=0.0237, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 19/68 [05:49<15:01,  0.05it/s, v_num=1, train_loss_step=0.0221, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 20/68 [06:07<14:42,  0.05it/s, v_num=1, train_loss_step=0.0221, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 20/68 [06:07<14:42,  0.05it/s, v_num=1, train_loss_step=0.0232, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 21/68 [06:26<14:24,  0.05it/s, v_num=1, train_loss_step=0.0232, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 21/68 [06:26<14:24,  0.05it/s, v_num=1, train_loss_step=0.0224, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 22/68 [06:44<14:05,  0.05it/s, v_num=1, train_loss_step=0.0224, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 22/68 [06:44<14:06,  0.05it/s, v_num=1, train_loss_step=0.021, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169] Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/68 [07:03<13:47,  0.05it/s, v_num=1, train_loss_step=0.021, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/68 [07:03<13:47,  0.05it/s, v_num=1, train_loss_step=0.022, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/68 [07:21<13:29,  0.05it/s, v_num=1, train_loss_step=0.022, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/68 [07:21<13:29,  0.05it/s, v_num=1, train_loss_step=0.0208, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/68 [07:39<13:10,  0.05it/s, v_num=1, train_loss_step=0.0208, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/68 [07:39<13:10,  0.05it/s, v_num=1, train_loss_step=0.0204, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 26/68 [07:58<12:52,  0.05it/s, v_num=1, train_loss_step=0.0204, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 26/68 [07:58<12:52,  0.05it/s, v_num=1, train_loss_step=0.020, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169] Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 27/68 [08:16<12:34,  0.05it/s, v_num=1, train_loss_step=0.020, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 27/68 [08:16<12:34,  0.05it/s, v_num=1, train_loss_step=0.0194, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 28/68 [08:35<12:15,  0.05it/s, v_num=1, train_loss_step=0.0194, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 28/68 [08:35<12:15,  0.05it/s, v_num=1, train_loss_step=0.0195, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/68 [08:53<11:57,  0.05it/s, v_num=1, train_loss_step=0.0195, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/68 [08:53<11:57,  0.05it/s, v_num=1, train_loss_step=0.0202, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/68 [09:11<11:38,  0.05it/s, v_num=1, train_loss_step=0.0202, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/68 [09:11<11:39,  0.05it/s, v_num=1, train_loss_step=0.0203, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 31/68 [09:30<11:20,  0.05it/s, v_num=1, train_loss_step=0.0203, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 31/68 [09:30<11:20,  0.05it/s, v_num=1, train_loss_step=0.0195, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 32/68 [09:48<11:02,  0.05it/s, v_num=1, train_loss_step=0.0195, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 32/68 [09:48<11:02,  0.05it/s, v_num=1, train_loss_step=0.0178, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 33/68 [10:07<10:43,  0.05it/s, v_num=1, train_loss_step=0.0178, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 33/68 [10:07<10:43,  0.05it/s, v_num=1, train_loss_step=0.0178, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/68 [10:25<10:25,  0.05it/s, v_num=1, train_loss_step=0.0178, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/68 [10:25<10:25,  0.05it/s, v_num=1, train_loss_step=0.0186, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/68 [10:44<10:07,  0.05it/s, v_num=1, train_loss_step=0.0186, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/68 [10:44<10:07,  0.05it/s, v_num=1, train_loss_step=0.0194, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/68 [11:02<09:49,  0.05it/s, v_num=1, train_loss_step=0.0194, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/68 [11:02<09:49,  0.05it/s, v_num=1, train_loss_step=0.0174, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 37/68 [11:21<09:30,  0.05it/s, v_num=1, train_loss_step=0.0174, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 37/68 [11:21<09:30,  0.05it/s, v_num=1, train_loss_step=0.0171, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 38/68 [11:39<09:12,  0.05it/s, v_num=1, train_loss_step=0.0171, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 38/68 [11:39<09:12,  0.05it/s, v_num=1, train_loss_step=0.0177, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 39/68 [11:57<08:53,  0.05it/s, v_num=1, train_loss_step=0.0177, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 39/68 [11:57<08:53,  0.05it/s, v_num=1, train_loss_step=0.0167, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/68 [12:16<08:35,  0.05it/s, v_num=1, train_loss_step=0.0167, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/68 [12:16<08:35,  0.05it/s, v_num=1, train_loss_step=0.0177, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/68 [12:34<08:16,  0.05it/s, v_num=1, train_loss_step=0.0177, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/68 [12:34<08:16,  0.05it/s, v_num=1, train_loss_step=0.0168, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42/68 [12:52<07:58,  0.05it/s, v_num=1, train_loss_step=0.0168, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42/68 [12:53<07:58,  0.05it/s, v_num=1, train_loss_step=0.018, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169] Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 43/68 [13:11<07:40,  0.05it/s, v_num=1, train_loss_step=0.018, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 43/68 [13:11<07:40,  0.05it/s, v_num=1, train_loss_step=0.0165, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 44/68 [13:29<07:21,  0.05it/s, v_num=1, train_loss_step=0.0165, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 44/68 [13:29<07:21,  0.05it/s, v_num=1, train_loss_step=0.0175, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 45/68 [13:48<07:03,  0.05it/s, v_num=1, train_loss_step=0.0175, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 45/68 [13:48<07:03,  0.05it/s, v_num=1, train_loss_step=0.0177, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/68 [14:06<06:44,  0.05it/s, v_num=1, train_loss_step=0.0177, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/68 [14:06<06:44,  0.05it/s, v_num=1, train_loss_step=0.0174, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 47/68 [14:24<06:26,  0.05it/s, v_num=1, train_loss_step=0.0174, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 47/68 [14:24<06:26,  0.05it/s, v_num=1, train_loss_step=0.0164, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 48/68 [14:43<06:08,  0.05it/s, v_num=1, train_loss_step=0.0164, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 48/68 [14:43<06:08,  0.05it/s, v_num=1, train_loss_step=0.0161, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/68 [15:01<05:49,  0.05it/s, v_num=1, train_loss_step=0.0161, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/68 [15:01<05:49,  0.05it/s, v_num=1, train_loss_step=0.0157, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 50/68 [15:20<05:31,  0.05it/s, v_num=1, train_loss_step=0.0157, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 50/68 [15:20<05:31,  0.05it/s, v_num=1, train_loss_step=0.0164, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/68 [15:38<05:12,  0.05it/s, v_num=1, train_loss_step=0.0164, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/68 [15:38<05:12,  0.05it/s, v_num=1, train_loss_step=0.0174, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52/68 [15:56<04:54,  0.05it/s, v_num=1, train_loss_step=0.0174, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52/68 [15:56<04:54,  0.05it/s, v_num=1, train_loss_step=0.0159, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 53/68 [16:15<04:36,  0.05it/s, v_num=1, train_loss_step=0.0159, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 53/68 [16:15<04:36,  0.05it/s, v_num=1, train_loss_step=0.0153, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 54/68 [16:33<04:17,  0.05it/s, v_num=1, train_loss_step=0.0153, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 54/68 [16:33<04:17,  0.05it/s, v_num=1, train_loss_step=0.0159, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 55/68 [16:52<03:59,  0.05it/s, v_num=1, train_loss_step=0.0159, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 55/68 [16:52<03:59,  0.05it/s, v_num=1, train_loss_step=0.0146, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/68 [17:10<03:40,  0.05it/s, v_num=1, train_loss_step=0.0146, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/68 [17:10<03:40,  0.05it/s, v_num=1, train_loss_step=0.0148, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/68 [17:28<03:22,  0.05it/s, v_num=1, train_loss_step=0.0148, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/68 [17:28<03:22,  0.05it/s, v_num=1, train_loss_step=0.0144, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58/68 [17:47<03:03,  0.05it/s, v_num=1, train_loss_step=0.0144, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58/68 [17:47<03:04,  0.05it/s, v_num=1, train_loss_step=0.0144, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 59/68 [18:05<02:45,  0.05it/s, v_num=1, train_loss_step=0.0144, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 59/68 [18:05<02:45,  0.05it/s, v_num=1, train_loss_step=0.0148, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 60/68 [18:23<02:27,  0.05it/s, v_num=1, train_loss_step=0.0148, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 60/68 [18:23<02:27,  0.05it/s, v_num=1, train_loss_step=0.0148, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 61/68 [18:42<02:08,  0.05it/s, v_num=1, train_loss_step=0.0148, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 61/68 [18:42<02:08,  0.05it/s, v_num=1, train_loss_step=0.0147, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 62/68 [19:00<01:50,  0.05it/s, v_num=1, train_loss_step=0.0147, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 62/68 [19:00<01:50,  0.05it/s, v_num=1, train_loss_step=0.0144, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 63/68 [19:19<01:31,  0.05it/s, v_num=1, train_loss_step=0.0144, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 63/68 [19:19<01:31,  0.05it/s, v_num=1, train_loss_step=0.0146, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 64/68 [19:37<01:13,  0.05it/s, v_num=1, train_loss_step=0.0146, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 64/68 [19:37<01:13,  0.05it/s, v_num=1, train_loss_step=0.013, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169] Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 65/68 [19:55<00:55,  0.05it/s, v_num=1, train_loss_step=0.013, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 65/68 [19:55<00:55,  0.05it/s, v_num=1, train_loss_step=0.0139, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 66/68 [20:14<00:36,  0.05it/s, v_num=1, train_loss_step=0.0139, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 66/68 [20:14<00:36,  0.05it/s, v_num=1, train_loss_step=0.0136, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [20:32<00:18,  0.05it/s, v_num=1, train_loss_step=0.0136, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [20:32<00:18,  0.05it/s, v_num=1, train_loss_step=0.0143, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [20:34<00:00,  0.06it/s, v_num=1, train_loss_step=0.0143, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [20:34<00:00,  0.06it/s, v_num=1, train_loss_step=0.0157, val_loss=0.0366, val_mm_error=275.0, train_loss_epoch=0.169]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:10<01:24,  0.09it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:21<01:14,  0.09it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:31<01:03,  0.09it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:42<00:53,  0.09it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:53<00:42,  0.09it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [01:03<00:31,  0.09it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:14<00:21,  0.09it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:24<00:10,  0.09it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:28<00:00,  0.10it/s][A
                                                                      [AEpoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [22:33<00:00,  0.05it/s, v_num=1, train_loss_step=0.0157, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.169]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [22:33<00:00,  0.05it/s, v_num=1, train_loss_step=0.0157, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 1:   0%|          | 0/68 [00:00<?, ?it/s, v_num=1, train_loss_step=0.0157, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]         Epoch 2:   0%|          | 0/68 [00:00<?, ?it/s, v_num=1, train_loss_step=0.0157, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   1%|â–         | 1/68 [00:18<20:37,  0.05it/s, v_num=1, train_loss_step=0.0157, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   1%|â–         | 1/68 [00:18<20:39,  0.05it/s, v_num=1, train_loss_step=0.0149, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   3%|â–Ž         | 2/68 [00:36<20:20,  0.05it/s, v_num=1, train_loss_step=0.0149, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   3%|â–Ž         | 2/68 [00:37<20:21,  0.05it/s, v_num=1, train_loss_step=0.013, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203] Epoch 2:   4%|â–         | 3/68 [00:55<19:59,  0.05it/s, v_num=1, train_loss_step=0.013, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   4%|â–         | 3/68 [00:55<20:00,  0.05it/s, v_num=1, train_loss_step=0.0166, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   6%|â–Œ         | 4/68 [01:13<19:42,  0.05it/s, v_num=1, train_loss_step=0.0166, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   6%|â–Œ         | 4/68 [01:13<19:42,  0.05it/s, v_num=1, train_loss_step=0.0129, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   7%|â–‹         | 5/68 [01:32<19:23,  0.05it/s, v_num=1, train_loss_step=0.0129, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   7%|â–‹         | 5/68 [01:32<19:23,  0.05it/s, v_num=1, train_loss_step=0.0135, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   9%|â–‰         | 6/68 [01:50<19:05,  0.05it/s, v_num=1, train_loss_step=0.0135, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:   9%|â–‰         | 6/68 [01:50<19:05,  0.05it/s, v_num=1, train_loss_step=0.0138, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  10%|â–ˆ         | 7/68 [02:09<18:46,  0.05it/s, v_num=1, train_loss_step=0.0138, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  10%|â–ˆ         | 7/68 [02:09<18:46,  0.05it/s, v_num=1, train_loss_step=0.0134, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  12%|â–ˆâ–        | 8/68 [02:27<18:27,  0.05it/s, v_num=1, train_loss_step=0.0134, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  12%|â–ˆâ–        | 8/68 [02:27<18:27,  0.05it/s, v_num=1, train_loss_step=0.0131, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  13%|â–ˆâ–Ž        | 9/68 [02:46<18:08,  0.05it/s, v_num=1, train_loss_step=0.0131, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  13%|â–ˆâ–Ž        | 9/68 [02:46<18:08,  0.05it/s, v_num=1, train_loss_step=0.0128, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  15%|â–ˆâ–        | 10/68 [03:04<17:50,  0.05it/s, v_num=1, train_loss_step=0.0128, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  15%|â–ˆâ–        | 10/68 [03:04<17:50,  0.05it/s, v_num=1, train_loss_step=0.0134, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  16%|â–ˆâ–Œ        | 11/68 [03:23<17:32,  0.05it/s, v_num=1, train_loss_step=0.0134, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  16%|â–ˆâ–Œ        | 11/68 [03:23<17:32,  0.05it/s, v_num=1, train_loss_step=0.0129, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  18%|â–ˆâ–Š        | 12/68 [03:41<17:14,  0.05it/s, v_num=1, train_loss_step=0.0129, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  18%|â–ˆâ–Š        | 12/68 [03:41<17:14,  0.05it/s, v_num=1, train_loss_step=0.013, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203] Epoch 2:  19%|â–ˆâ–‰        | 13/68 [04:00<16:56,  0.05it/s, v_num=1, train_loss_step=0.013, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  19%|â–ˆâ–‰        | 13/68 [04:00<16:56,  0.05it/s, v_num=1, train_loss_step=0.0118, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  21%|â–ˆâ–ˆ        | 14/68 [04:18<16:37,  0.05it/s, v_num=1, train_loss_step=0.0118, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  21%|â–ˆâ–ˆ        | 14/68 [04:18<16:38,  0.05it/s, v_num=1, train_loss_step=0.0126, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  22%|â–ˆâ–ˆâ–       | 15/68 [04:37<16:19,  0.05it/s, v_num=1, train_loss_step=0.0126, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  22%|â–ˆâ–ˆâ–       | 15/68 [04:37<16:19,  0.05it/s, v_num=1, train_loss_step=0.012, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203] Epoch 2:  24%|â–ˆâ–ˆâ–Ž       | 16/68 [04:55<16:01,  0.05it/s, v_num=1, train_loss_step=0.012, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  24%|â–ˆâ–ˆâ–Ž       | 16/68 [04:55<16:01,  0.05it/s, v_num=1, train_loss_step=0.0116, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  25%|â–ˆâ–ˆâ–Œ       | 17/68 [05:14<15:42,  0.05it/s, v_num=1, train_loss_step=0.0116, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  25%|â–ˆâ–ˆâ–Œ       | 17/68 [05:14<15:42,  0.05it/s, v_num=1, train_loss_step=0.0119, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  26%|â–ˆâ–ˆâ–‹       | 18/68 [05:32<15:24,  0.05it/s, v_num=1, train_loss_step=0.0119, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  26%|â–ˆâ–ˆâ–‹       | 18/68 [05:32<15:24,  0.05it/s, v_num=1, train_loss_step=0.0119, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 19/68 [05:51<15:05,  0.05it/s, v_num=1, train_loss_step=0.0119, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 19/68 [05:51<15:06,  0.05it/s, v_num=1, train_loss_step=0.0113, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 20/68 [06:09<14:47,  0.05it/s, v_num=1, train_loss_step=0.0113, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 20/68 [06:09<14:47,  0.05it/s, v_num=1, train_loss_step=0.0118, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  31%|â–ˆâ–ˆâ–ˆ       | 21/68 [06:28<14:28,  0.05it/s, v_num=1, train_loss_step=0.0118, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  31%|â–ˆâ–ˆâ–ˆ       | 21/68 [06:28<14:28,  0.05it/s, v_num=1, train_loss_step=0.0112, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 22/68 [06:46<14:10,  0.05it/s, v_num=1, train_loss_step=0.0112, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 22/68 [06:46<14:10,  0.05it/s, v_num=1, train_loss_step=0.0116, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/68 [07:05<13:51,  0.05it/s, v_num=1, train_loss_step=0.0116, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/68 [07:05<13:52,  0.05it/s, v_num=1, train_loss_step=0.0113, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/68 [07:23<13:33,  0.05it/s, v_num=1, train_loss_step=0.0113, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/68 [07:23<13:33,  0.05it/s, v_num=1, train_loss_step=0.0113, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/68 [07:42<13:15,  0.05it/s, v_num=1, train_loss_step=0.0113, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/68 [07:42<13:15,  0.05it/s, v_num=1, train_loss_step=0.0111, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 26/68 [08:00<12:56,  0.05it/s, v_num=1, train_loss_step=0.0111, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 26/68 [08:00<12:56,  0.05it/s, v_num=1, train_loss_step=0.0112, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 27/68 [08:19<12:38,  0.05it/s, v_num=1, train_loss_step=0.0112, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 27/68 [08:19<12:38,  0.05it/s, v_num=1, train_loss_step=0.0108, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 28/68 [08:37<12:19,  0.05it/s, v_num=1, train_loss_step=0.0108, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 28/68 [08:37<12:19,  0.05it/s, v_num=1, train_loss_step=0.0107, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/68 [08:56<12:01,  0.05it/s, v_num=1, train_loss_step=0.0107, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/68 [08:56<12:01,  0.05it/s, v_num=1, train_loss_step=0.0102, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/68 [09:14<11:42,  0.05it/s, v_num=1, train_loss_step=0.0102, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/68 [09:14<11:42,  0.05it/s, v_num=1, train_loss_step=0.0104, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 31/68 [09:33<11:24,  0.05it/s, v_num=1, train_loss_step=0.0104, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 31/68 [09:33<11:24,  0.05it/s, v_num=1, train_loss_step=0.0104, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 32/68 [09:51<11:05,  0.05it/s, v_num=1, train_loss_step=0.0104, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 32/68 [09:51<11:05,  0.05it/s, v_num=1, train_loss_step=0.0103, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 33/68 [10:10<10:47,  0.05it/s, v_num=1, train_loss_step=0.0103, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 33/68 [10:10<10:47,  0.05it/s, v_num=1, train_loss_step=0.0106, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/68 [10:28<10:28,  0.05it/s, v_num=1, train_loss_step=0.0106, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/68 [10:28<10:28,  0.05it/s, v_num=1, train_loss_step=0.0105, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/68 [10:47<10:10,  0.05it/s, v_num=1, train_loss_step=0.0105, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/68 [10:47<10:10,  0.05it/s, v_num=1, train_loss_step=0.00939, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/68 [11:05<09:51,  0.05it/s, v_num=1, train_loss_step=0.00939, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/68 [11:05<09:51,  0.05it/s, v_num=1, train_loss_step=0.0104, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203] Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 37/68 [11:24<09:33,  0.05it/s, v_num=1, train_loss_step=0.0104, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 37/68 [11:24<09:33,  0.05it/s, v_num=1, train_loss_step=0.0101, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 38/68 [11:42<09:14,  0.05it/s, v_num=1, train_loss_step=0.0101, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 38/68 [11:42<09:14,  0.05it/s, v_num=1, train_loss_step=0.010, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203] Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 39/68 [12:01<08:56,  0.05it/s, v_num=1, train_loss_step=0.010, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 39/68 [12:01<08:56,  0.05it/s, v_num=1, train_loss_step=0.0101, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/68 [12:19<08:37,  0.05it/s, v_num=1, train_loss_step=0.0101, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/68 [12:19<08:37,  0.05it/s, v_num=1, train_loss_step=0.00971, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/68 [12:37<08:19,  0.05it/s, v_num=1, train_loss_step=0.00971, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/68 [12:37<08:19,  0.05it/s, v_num=1, train_loss_step=0.00944, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42/68 [12:56<08:00,  0.05it/s, v_num=1, train_loss_step=0.00944, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42/68 [12:56<08:00,  0.05it/s, v_num=1, train_loss_step=0.00939, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 43/68 [13:14<07:42,  0.05it/s, v_num=1, train_loss_step=0.00939, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 43/68 [13:14<07:42,  0.05it/s, v_num=1, train_loss_step=0.0094, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203] Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 44/68 [13:33<07:23,  0.05it/s, v_num=1, train_loss_step=0.0094, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 44/68 [13:33<07:23,  0.05it/s, v_num=1, train_loss_step=0.0095, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 45/68 [13:51<07:05,  0.05it/s, v_num=1, train_loss_step=0.0095, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 45/68 [13:51<07:05,  0.05it/s, v_num=1, train_loss_step=0.00893, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/68 [14:10<06:46,  0.05it/s, v_num=1, train_loss_step=0.00893, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/68 [14:10<06:46,  0.05it/s, v_num=1, train_loss_step=0.00951, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 47/68 [14:28<06:28,  0.05it/s, v_num=1, train_loss_step=0.00951, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 47/68 [14:28<06:28,  0.05it/s, v_num=1, train_loss_step=0.00873, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 48/68 [14:47<06:09,  0.05it/s, v_num=1, train_loss_step=0.00873, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 48/68 [14:47<06:09,  0.05it/s, v_num=1, train_loss_step=0.00897, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/68 [15:05<05:51,  0.05it/s, v_num=1, train_loss_step=0.00897, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/68 [15:05<05:51,  0.05it/s, v_num=1, train_loss_step=0.00948, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 50/68 [15:24<05:32,  0.05it/s, v_num=1, train_loss_step=0.00948, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 50/68 [15:24<05:32,  0.05it/s, v_num=1, train_loss_step=0.00847, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/68 [15:42<05:14,  0.05it/s, v_num=1, train_loss_step=0.00847, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/68 [15:42<05:14,  0.05it/s, v_num=1, train_loss_step=0.00879, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52/68 [16:01<04:55,  0.05it/s, v_num=1, train_loss_step=0.00879, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52/68 [16:01<04:55,  0.05it/s, v_num=1, train_loss_step=0.00867, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 53/68 [16:19<04:37,  0.05it/s, v_num=1, train_loss_step=0.00867, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 53/68 [16:19<04:37,  0.05it/s, v_num=1, train_loss_step=0.00935, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 54/68 [16:38<04:18,  0.05it/s, v_num=1, train_loss_step=0.00935, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 54/68 [16:38<04:18,  0.05it/s, v_num=1, train_loss_step=0.00839, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 55/68 [16:56<04:00,  0.05it/s, v_num=1, train_loss_step=0.00839, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 55/68 [16:57<04:00,  0.05it/s, v_num=1, train_loss_step=0.00832, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/68 [17:15<03:41,  0.05it/s, v_num=1, train_loss_step=0.00832, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/68 [17:15<03:41,  0.05it/s, v_num=1, train_loss_step=0.00811, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/68 [17:33<03:23,  0.05it/s, v_num=1, train_loss_step=0.00811, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/68 [17:33<03:23,  0.05it/s, v_num=1, train_loss_step=0.00832, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58/68 [17:52<03:04,  0.05it/s, v_num=1, train_loss_step=0.00832, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58/68 [17:52<03:04,  0.05it/s, v_num=1, train_loss_step=0.00861, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 59/68 [18:10<02:46,  0.05it/s, v_num=1, train_loss_step=0.00861, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 59/68 [18:10<02:46,  0.05it/s, v_num=1, train_loss_step=0.00867, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 60/68 [18:29<02:27,  0.05it/s, v_num=1, train_loss_step=0.00867, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 60/68 [18:29<02:27,  0.05it/s, v_num=1, train_loss_step=0.00824, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 61/68 [18:47<02:09,  0.05it/s, v_num=1, train_loss_step=0.00824, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 61/68 [18:47<02:09,  0.05it/s, v_num=1, train_loss_step=0.0079, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203] Epoch 2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 62/68 [19:06<01:50,  0.05it/s, v_num=1, train_loss_step=0.0079, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 62/68 [19:06<01:50,  0.05it/s, v_num=1, train_loss_step=0.00847, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 63/68 [19:24<01:32,  0.05it/s, v_num=1, train_loss_step=0.00847, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 63/68 [19:24<01:32,  0.05it/s, v_num=1, train_loss_step=0.00796, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 64/68 [19:43<01:13,  0.05it/s, v_num=1, train_loss_step=0.00796, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 64/68 [19:44<01:14,  0.05it/s, v_num=1, train_loss_step=0.00843, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 65/68 [20:02<00:55,  0.05it/s, v_num=1, train_loss_step=0.00843, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 65/68 [20:02<00:55,  0.05it/s, v_num=1, train_loss_step=0.00827, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 66/68 [20:21<00:37,  0.05it/s, v_num=1, train_loss_step=0.00827, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 66/68 [20:21<00:37,  0.05it/s, v_num=1, train_loss_step=0.00858, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [20:39<00:18,  0.05it/s, v_num=1, train_loss_step=0.00858, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [20:39<00:18,  0.05it/s, v_num=1, train_loss_step=0.00802, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [20:41<00:00,  0.05it/s, v_num=1, train_loss_step=0.00802, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [20:41<00:00,  0.05it/s, v_num=1, train_loss_step=0.00935, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]
Validation: |          | 0/? [00:00<?, ?it/s][ATraceback (most recent call last):
  File "/sw/arch/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/sw/arch/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home5/flaitenberger/Cephalometry/experiment/__main__.py", line 165, in <module>
    results = run(args, seed=run_idx)[0]
  File "/gpfs/home5/flaitenberger/Cephalometry/experiment/__main__.py", line 140, in run
    trainer.fit(model=model, datamodule=datamodule)
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 139, in run
    self.on_advance_end(data_fetcher)
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 287, in on_advance_end
    self.val_loop.run()
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 114, in run
    self.on_run_start()
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 245, in on_run_start
    self._on_evaluation_epoch_start()
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 325, in _on_evaluation_epoch_start
    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)
  File "/home/flaitenberger/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 208, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/gpfs/home5/flaitenberger/Cephalometry/experiment/loggers/ImagePredictionLogger.py", line 44, in on_validation_epoch_start
    plt.imread('figure.png')
  File "/sw/arch/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages/matplotlib/pyplot.py", line 2139, in imread
    return matplotlib.image.imread(fname, format)
  File "/sw/arch/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages/matplotlib/image.py", line 1560, in imread
    with img_open(fname) as image:
  File "/sw/arch/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages/PIL/ImageFile.py", line 112, in __init__
    self._open()
  File "/sw/arch/RHEL8/EB_production/2022/software/Anaconda3/2022.05/lib/python3.9/site-packages/PIL/PngImagePlugin.py", line 676, in _open
    raise SyntaxError("not a PNG file")
SyntaxError: not a PNG file
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [20:47<00:00,  0.05it/s, v_num=1, train_loss_step=0.00935, val_loss=0.0149, val_mm_error=277.0, train_loss_epoch=0.0203]

                                             [A
JOB STATISTICS
==============
Job ID: 5386460
Cluster: snellius
User/Group: flaitenberger/flaitenberger
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 20:13:12 core-walltime
Job Wall-clock time: 01:07:24
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
